{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acknowledgement:\n",
    "# https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"indobenchmark/indobert-base-p1\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('augment_subtitute.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Unnamed: 0', 'sub-category', 'has_exclamation', 'has_question', 'has_number', 'mark_count', 'total_character', 'original_title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['title', 'label_score']]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords is the words that often show up in a sentence\n",
    "stop = stopwords.words('indonesian')\n",
    "stop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from: https://www.kaggle.com/code/ahmadalqawasmeh/nlp-text-clustering-a-simple-guide\n",
    "# modified by us to use Sastrawi stemmer \n",
    "\n",
    "\n",
    "## This function to clean the text in the col (text-col)\n",
    "def clean_text(text):\n",
    "    text=str(text).lower() #x``\n",
    "    text=re.sub('\\d+', '', text) #removes numbers\n",
    "    text=re.sub('\\[.*?\\]', '', text) #removes HTML tags\n",
    "    text=re.sub('https?://\\S+|www\\.\\S+', '', text) #removes url\n",
    "    text=re.sub(r\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", \"\", text) #removes emojis\n",
    "    text=re.sub('[%s]' % re.escape(string.punctuation),'',text) #removes punctuations\n",
    "    # text = [ps.stem(word) for word in text.split(' ') if not word in set(stop) ]  ## remove all stopwords from the text, apply steming on words\n",
    "    text = [stemmer.stem(word) for word in text.split(' ') if not word in stop] #remove stopwords then apply stemmer\n",
    "    text = ' '.join(text) ## join the words seperated by spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting first title\n",
    "first_title = df[\"title\"][0]\n",
    "print(f\"Before: {first_title}\")\n",
    "\n",
    "#clean whole title\n",
    "df['title']=df['title'].apply(clean_text)\n",
    "\n",
    "# notice the difference before and after cleaning\n",
    "first_title = df[\"title\"][0]\n",
    "print(f\"After: {first_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using tokenizer\n",
    "\n",
    "print(' Original: ', df[\"title\"][0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(df[\"title\"][0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df[\"title\"][0])))\n",
    "\n",
    "print('Length of the token:',len(tokenizer.tokenize(df[\"title\"][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the longset sentence and it's token length (for attention mask)\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sentence in df[\"title\"]:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens in the beginning and end.\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length to get the max length of sentence\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max array length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every title...\n",
    "for title in df[\"title\"]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        title,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 33,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(df['label_score'].values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that out dataset has 14757 rows\n",
    "\n",
    "print('input id shape:', input_ids.shape)\n",
    "print('attention_mask shape:', attention_masks.shape)\n",
    "print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input id sample\n",
    "input_ids[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask sample (notice that 0 means there is no word there. We do this to make the array to be the same length)\n",
    "attention_masks[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels sample\n",
    "labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# looking at dataset\n",
    "dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test (here we use 80-10-10 split)\n",
    "# Define the ratios for the train, validation, and test sets\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup device agnostic code\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on cpu or gpu (use gpu if available)\n",
    "model = model.to(device)\n",
    "\n",
    "# this is the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scheduler,\n",
    "               device=device):\n",
    "    #setup total training loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #training mode: activated\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack the training batch from the dataloader, put to device (gpu or cpu)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        #compute loss\n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "    #count average from accumulated train loss\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scheduler,\n",
    "               device=device):\n",
    "\n",
    "    #evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "    best_recall_score = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        with torch.inference_mode():\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        #compute and accumulate loss\n",
    "        loss = output.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        #get the logit\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        #get the label (notice that there are only 2 possible label so we take the max)\n",
    "        val_predictions.extend(np.argmax(logits, axis=1).tolist())\n",
    "        val_labels.extend(label_ids.tolist())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "\n",
    "    #get accuracy and recall score\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    val_recall = recall_score(val_labels, val_predictions, average='binary')\n",
    "\n",
    "    if val_recall > best_recall_score:\n",
    "        torch.save(model, 'indobert_fine-tuned_augmented')\n",
    "        best_recall_score = val_recall\n",
    "\n",
    "    return avg_val_loss, val_accuracy, val_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga usah dicoba\n",
    "# 2e-7\n",
    "# 0.000002=2e-6\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "lr = 2e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr = lr, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                              eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                              )\n",
    "\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"IndoBert Finetuning\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"indobenchmark/indobert-base-p1\",\n",
    "    \"dataset\": \"augment_subtitute\",\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\":batch_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for graphing purposes:\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "avg_val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "val_recall_list = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(\"-\"*10)\n",
    "    train_loss = train_step(model=model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, device=device)\n",
    "    avg_val_loss, val_accuracy, val_recall = val_step(model=model, dataloader=validation_dataloader, optimizer=optimizer, scheduler=scheduler, device=device)\n",
    "    print(f\"Train Loss: {train_loss*100:.4f} | Avg validation loss: {avg_val_loss*100:.4f} | Validation accuracy: {val_accuracy*100:.4f} | Validation Recall {val_recall*100:.6f}\")\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"Train Loss\": train_loss, \"Avg validation loss\": avg_val_loss,\n",
    "               \"Validation accuracy\":val_accuracy, \"Validation Recall\":val_recall,\n",
    "               })\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    train_loss_list.append(train_loss)\n",
    "    avg_val_loss_list.append(avg_val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "    val_recall_list.append(val_recall)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = [i for i in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(epoch_list, np.array(torch.tensor(train_loss_list)), label= \"Train Loss\")\n",
    "plt.plot(epoch_list, np.array(torch.tensor(avg_val_loss_list)), label= \"Average Validation Loss\")\n",
    "plt.plot(epoch_list, np.array(torch.tensor(val_accuracy_list)), label= \"Validation Accuracy\")\n",
    "plt.plot(epoch_list, np.array(torch.tensor(val_recall_list)), label= \"Validation Recall\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Loss, Accuracy, and Recall Progress over fine tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving the model the data it never seen for testing\n",
    "avg_test_loss, test_accuracy, test_recall = val_step(model=model, dataloader=test_dataloader, optimizer=optimizer, scheduler=scheduler, device=device)\n",
    "\n",
    "print(f\"Avg Test loss: {avg_test_loss*100:.4f} | Test accuracy: {test_accuracy*100:.4f} | Test Recall {test_recall*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
